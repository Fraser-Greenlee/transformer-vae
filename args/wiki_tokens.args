dataset_name=datasets/wiki_sentences.py
set_seq_size=64
generate_max_len=64
tokenizer_name=gpt2-base
dont_clean_up_tokenization_spaces
validation_name=train
max_validation_size=100
sample_from_latent